## Codebase Patterns
- This is a Python scraper project in the /scraper directory
- Main scraping logic in scraper/src/scraper.py (CarnaMapaScraper class)
- Geocoding in scraper/src/geocoder.py (Geocoder class with fallback chain and caching)
- Address normalization in scraper/src/address_normalizer.py (normalize_address, extract_landmark)
- City configs in scraper/src/config.py (CITIES dict with name, url, slug)
- Output files in scraper/output/{city-slug}.json (GeoJSON format)
- Cache stored in scraper/cache/geocoding_cache.json (new format: {coords, provider, timestamp})
- Uses geopy library for geocoding (supports Nominatim and GoogleV3)
- Uses beautifulsoup4 for HTML parsing
- Uses requests for HTTP
- Environment variables loaded via python-dotenv
- TypeScript checking equivalent: python -m py_compile <file>
- 9 cities configured: sao-paulo, rio-de-janeiro, belo-horizonte, salvador, florianopolis, recife-olinda, brasilia, porto-alegre, fortaleza

- Use python3 (not python) for Python commands on macOS
- Typecheck for Python files: python3 -m py_compile <file>
- Geocoder.geocode_with_fallback() is the main geocoding method (4-step fallback chain)
- Geocoder.geocode_with_google_only() for retrying failed Nominatim lookups
- GeocodingPool in scraper/src/geocoding_pool.py for parallel geocoding with rate limiting
- Skip checker in scraper/src/skip_checker.py (load_existing_event_ids, should_skip_event)
- Pipeline orchestrator in scraper/src/pipeline.py (Pipeline class, main entry point)
- Run pipeline: cd scraper && python3 src/pipeline.py [--cities X,Y] [--dry-run] [--retry-failed]

---

## 2026-01-28 - US-001
- What was implemented: Added Google Maps API configuration support for geocoding fallback
- Files changed:
  - scraper/.env.example (updated with GOOGLE_MAPS_API_KEY, GEOCODING_GOOGLE_ENABLED, GEOCODING_NOMINATIM_CONCURRENCY, GEOCODING_GOOGLE_CONCURRENCY)
  - scraper/src/config.py (added environment variable loading with get_bool_env and get_int_env helpers)
- **Learnings for future iterations:**
  - python-dotenv load_dotenv() should be called at module import time in config.py
  - Use helper functions for type conversion from env vars (bool and int)
  - Boolean env vars accept 'true', '1', 'yes', 'on' (case-insensitive)
  - Default values: GEOCODING_GOOGLE_ENABLED=True, NOMINATIM_CONCURRENCY=1, GOOGLE_CONCURRENCY=10
---

## 2026-01-28 - US-002
- What was implemented: Created address normalization module with two functions for improving geocoding success
- Files changed:
  - scraper/src/address_normalizer.py (new file)
- **Learnings for future iterations:**
  - normalize_address() handles: parenthetical notes, zip codes (5-8 digits), 'Brasil' suffix, 's/n' (sem número), 'Centro Histórico' -> 'Centro'
  - extract_landmark() can extract landmarks like 'Praça da Sé', 'Parque Ibirapuera', etc. for fallback geocoding queries
  - Brazilian zip code formats: 04784-145, 04784145, 04784 145 (5 digits + optional separator + 3 digits)
  - Common patterns removed: 'próximo', 'perto', 'ao lado', 'em frente', 'esquina'
---

## 2026-01-28 - US-003
- What was implemented: Refactored geocoder.py to support a 4-step fallback chain for maximum geocoding success rate
- Files changed:
  - scraper/src/geocoder.py (major refactor)
- **Learnings for future iterations:**
  - geocode_with_fallback() implements 4-step chain: Nominatim full → Google full → Nominatim simplified → Google simplified
  - Cache format now includes provider metadata: {'coords': [lon, lat], 'provider': 'nominatim'|'google', 'timestamp': ISO}
  - Legacy cache format (just [lon, lat] array) is still supported for backward compatibility
  - _get_cached_result() handles both old and new cache formats
  - geocode_with_google_only() is available for retry scenarios (US-008 will use this)
  - Nominatim rate limiting (1 req/sec) is built into geocode_with_fallback via time.sleep(1)
  - get_stats() now returns provider breakdown: nominatim_hits, google_hits, legacy_hits
  - Legacy geocode() method delegates to geocode_with_fallback() for backward compatibility
---

## 2026-01-28 - US-004
- What was implemented: Created GeocodingPool class for parallel geocoding with rate limiting
- Files changed:
  - scraper/src/geocoding_pool.py (new file)
- **Learnings for future iterations:**
  - GeocodingPool accepts List[Tuple[str, str]] of (address, city) pairs
  - Uses ThreadPoolExecutor for parallel processing
  - Nominatim rate limiting enforced via threading.Lock + time-based throttle (_throttle_nominatim method)
  - Results keyed by "address|city" string format for thread-safe dict access
  - Progress logging triggers at: every 10 addresses, every 10% completion, or final completion
  - get_results_by_address() method available for (address, city) tuple keys
  - Imports GEOCODING_NOMINATIM_CONCURRENCY and GEOCODING_GOOGLE_CONCURRENCY from config
  - Max workers set to max(NOMINATIM_CONCURRENCY, GOOGLE_CONCURRENCY) for optimal parallelism
- CityScraper class in scraper/src/city_scraper.py for parallel event scraping within a city
- CityScraper.scrape() returns list of event dicts (raw data without geocoding)
- MAX_WORKERS = 5 for concurrent event page fetches per city
- Skip logic integrated: load_existing_event_ids() + should_skip_event()

---

## 2026-01-28 - US-005
- What was implemented: Created skip_checker module for avoiding re-processing already scraped events
- Files changed:
  - scraper/src/skip_checker.py (new file)
- **Learnings for future iterations:**
  - load_existing_event_ids(city_slug) reads output/{city}.json and returns set of event IDs with valid coordinates
  - Valid coordinates means geometry.coordinates is not None (null in JSON)
  - should_skip_event(event_id, existing_ids) is a simple set membership check
  - Both functions handle edge cases: missing files return empty set, JSON parse errors return empty set
  - Event IDs match the "id" field in GeoJSON features (e.g., "ensaio-aberto-bloco-reciclar-sp-27-01-26")
  - Output file path: scraper/output/{city-slug}.json
---

## 2026-01-28 - US-006
- What was implemented: Created CityScraper class for parallel scraping of event pages within a single city
- Files changed:
  - scraper/src/city_scraper.py (new file)
- **Learnings for future iterations:**
  - CityScraper.scrape() returns raw event data WITHOUT geocoding - geocoding is done later by pipeline
  - MAX_WORKERS = 5 for concurrent ThreadPoolExecutor (respects website)
  - Thread-safe logging via _log_lock and _log() method with [city_slug] prefix
  - Integrates skip_checker: load_existing_event_ids() before scraping, should_skip_event() to filter URLs
  - Partial results preserved on error: try/except around each event, errors logged but don't stop city
  - Stats tracking: events_found, events_skipped, events_scraped, errors
  - Events have needs_geocoding=True and geocoding_query set for later pipeline batch geocoding
---

## 2026-01-28 - US-007
- What was implemented: Created pipeline.py as the main entry point for orchestrating all scraping steps
- Files changed:
  - scraper/src/pipeline.py (new file)
- **Learnings for future iterations:**
  - Pipeline class orchestrates 4 steps: scrape cities -> collect addresses -> batch geocode -> generate output
  - CLI flags: --cities (comma-separated), --dry-run, --force-refresh, --retry-failed
  - Step 1 uses ThreadPoolExecutor with one thread per city for parallel scraping
  - Step 2 collects unique (address, city) tuples needing geocoding using a Set
  - Step 3 uses GeocodingPool for batch geocoding with shared Geocoder instance for cache reuse
  - Step 4 applies geocoding results to events and generates GeoJSON output per city
  - Geocode results keyed by "address|city" string for matching back to events
  - print_summary() shows cities scraped, events found/scraped/skipped, geocoding stats
  - Logs go to logs/pipeline.log via setup_logging()
  - Events that fail geocoding retain needs_geocoding=True in output for later retry
---

## 2026-01-28 - US-008
- What was implemented: Added --retry-failed flag handling to pipeline.py for retrying failed geocoding
- Files changed:
  - scraper/src/pipeline.py (updated with run_retry_failed method)
- **Learnings for future iterations:**
  - run_retry_failed() scans output/*.json for entries with needs_geocoding: true
  - Uses glob.glob() to find all output JSON files
  - Calls geocoder.geocode_with_google_only() to skip Nominatim (since it already failed)
  - Updates files in-place: sets coordinates, clears needs_geocoding flag, clears geocoding_query
  - Prints retry report with total_retried, succeeded, still_failing counts
  - If --retry-failed is set, run() calls run_retry_failed() and returns early (skips normal scraping)
  - Respects --dry-run flag: won't write files if dry-run is enabled
---

